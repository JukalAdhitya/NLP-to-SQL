{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"wikisql\", trust_remote_code=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T09:45:21.089912Z","iopub.execute_input":"2024-11-09T09:45:21.090298Z","iopub.status.idle":"2024-11-09T09:45:23.530094Z","shell.execute_reply.started":"2024-11-09T09:45:21.090256Z","shell.execute_reply":"2024-11-09T09:45:23.529134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the structure of the dataset\nprint(dataset)\n\n# Check a sample from the training data\nprint(dataset['train'][15])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T09:45:23.531842Z","iopub.execute_input":"2024-11-09T09:45:23.532878Z","iopub.status.idle":"2024-11-09T09:45:23.538841Z","shell.execute_reply.started":"2024-11-09T09:45:23.532832Z","shell.execute_reply":"2024-11-09T09:45:23.538015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_data(example):\n    # Directly use the 'human_readable' query as the target\n    sql_query = example['sql']['human_readable']\n    \n    return {\n        \"input_text\": example[\"question\"],  # The natural language question\n        \"target_text\": sql_query  # The human-readable SQL query\n    }\n\n# Apply preprocessing to the train and validation sets\ntrain_data = dataset[\"train\"].map(preprocess_data, remove_columns=[\"question\", \"sql\"])\nval_data = dataset[\"validation\"].map(preprocess_data, remove_columns=[\"question\", \"sql\"])\n\n# Check a preprocessed sample\nprint(train_data[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T09:45:23.539947Z","iopub.execute_input":"2024-11-09T09:45:23.540623Z","iopub.status.idle":"2024-11-09T09:45:23.563302Z","shell.execute_reply.started":"2024-11-09T09:45:23.540578Z","shell.execute_reply":"2024-11-09T09:45:23.562437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BartTokenizer\n\n# Load the tokenizer for BART\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n\n# Tokenization function\ndef tokenize_data(example):\n    # Tokenize both input and target texts\n    inputs = tokenizer(example['input_text'], padding='max_length', truncation=True, max_length=512)\n    targets = tokenizer(example['target_text'], padding='max_length', truncation=True, max_length=64)\n    \n    # Return the tokenized version\n    return {\n        'input_ids': inputs['input_ids'],\n        'attention_mask': inputs['attention_mask'],\n        'labels': targets['input_ids']\n    }\n\n# Tokenize the train and validation datasets\ntrain_data = train_data.map(tokenize_data, remove_columns=['input_text', 'target_text'])\nval_data = val_data.map(tokenize_data, remove_columns=['input_text', 'target_text'])\n\n# Check a tokenized sample\nprint(train_data[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T09:45:23.565665Z","iopub.execute_input":"2024-11-09T09:45:23.565951Z","iopub.status.idle":"2024-11-09T09:45:30.424004Z","shell.execute_reply.started":"2024-11-09T09:45:23.565920Z","shell.execute_reply":"2024-11-09T09:45:30.423025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_data[0])  # Check a sample after tokenization\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T09:45:30.425424Z","iopub.execute_input":"2024-11-09T09:45:30.426105Z","iopub.status.idle":"2024-11-09T09:45:30.432399Z","shell.execute_reply.started":"2024-11-09T09:45:30.426051Z","shell.execute_reply":"2024-11-09T09:45:30.431524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Disable Weights and Biases logging\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T09:45:30.433634Z","iopub.execute_input":"2024-11-09T09:45:30.433958Z","iopub.status.idle":"2024-11-09T09:45:30.443142Z","shell.execute_reply.started":"2024-11-09T09:45:30.433923Z","shell.execute_reply":"2024-11-09T09:45:30.442417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BartForConditionalGeneration, Trainer, TrainingArguments\n\n# Load the BART model for conditional generation\nmodel = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=10,\n    per_device_eval_batch_size=10,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    save_steps=500,\n    load_best_model_at_end=True,\n    report_to=\"none\",\n    fp16=True,\n    dataloader_num_workers=8,  # Increase number of workers\n)\n\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,                        # The model to train\n    args=training_args,                 # Training arguments\n    train_dataset=train_data,           # The training dataset\n    eval_dataset=val_data,              # The validation dataset\n)\n\n# Train the model\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T09:45:30.444247Z","iopub.execute_input":"2024-11-09T09:45:30.444602Z","iopub.status.idle":"2024-11-09T11:43:43.575795Z","shell.execute_reply.started":"2024-11-09T09:45:30.444566Z","shell.execute_reply":"2024-11-09T11:43:43.574792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Clear cache in PyTorch\ntorch.cuda.empty_cache()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T11:43:43.577166Z","iopub.execute_input":"2024-11-09T11:43:43.577844Z","iopub.status.idle":"2024-11-09T11:43:43.977268Z","shell.execute_reply.started":"2024-11-09T11:43:43.577807Z","shell.execute_reply":"2024-11-09T11:43:43.976259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained model and tokenizer\nmodel.save_pretrained('/kaggle/working/final_model')\ntokenizer.save_pretrained('/kaggle/working/final_model')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T11:44:54.856251Z","iopub.execute_input":"2024-11-09T11:44:54.857019Z","iopub.status.idle":"2024-11-09T11:44:57.854591Z","shell.execute_reply.started":"2024-11-09T11:44:54.856979Z","shell.execute_reply":"2024-11-09T11:44:57.853560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the model using the Trainer\ntrainer.evaluate()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T11:46:08.489190Z","iopub.execute_input":"2024-11-09T11:46:08.489867Z","iopub.status.idle":"2024-11-09T11:51:39.041621Z","shell.execute_reply.started":"2024-11-09T11:46:08.489823Z","shell.execute_reply":"2024-11-09T11:51:39.040713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Create a ZIP of the final model folder (or any folder you want to download)\nshutil.make_archive('/kaggle/working/final_model', 'zip', '/kaggle/working/final_model')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T12:00:08.099277Z","iopub.execute_input":"2024-11-09T12:00:08.100353Z","iopub.status.idle":"2024-11-09T12:01:41.099307Z","shell.execute_reply.started":"2024-11-09T12:00:08.100292Z","shell.execute_reply":"2024-11-09T12:01:41.098336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BartForConditionalGeneration, BartTokenizer\nimport sqlite3\n\n# Define the tables in a string format (for model input)\nemployees_table = \"\"\"\nEmployees table:\nID | Name   | Department  | Salary\n1  | John   | HR          | 50000\n2  | Alice  | Engineering | 70000\n3  | Bob    | Engineering | 75000\n4  | Clara  | HR          | 55000\n5  | David  | Engineering | 80000\n\"\"\"\n\ndepartments_table = \"\"\"\nDepartments table:\nID | Department_Name | Location\n1  | HR              | New York\n2  | Engineering     | San Francisco\n3  | Marketing       | Chicago\n\"\"\"\n\n# Define the question\nquestion = \"What is the average salary of employees in each department located in New York?\"\n\n# Combine the tables and question into a single input string\ninput_text = employees_table + \"\\n\" + departments_table + \"\\nQuestion: \" + question\n\n# Load the trained model and tokenizer (replace with your model path)\nmodel = BartForConditionalGeneration.from_pretrained('/kaggle/input/bart/transformers/default/1')  # Replace with your model\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n\n# Tokenize the input\ninputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n\n# Generate SQL query from the model\noutput = model.generate(inputs['input_ids'], max_length=100, num_beams=5, early_stopping=True)\n\n# Decode the generated SQL query\ngenerated_sql = tokenizer.decode(output[0], skip_special_tokens=True)\n\n\n# Display the generated SQL query\nprint(\"Generated SQL Query:\", generated_sql)\n\n# Set up the SQLite in-memory database and insert data into both tables\nconn = sqlite3.connect(':memory:')  # Using in-memory database for testing\ncursor = conn.cursor()\n\n# Create Employees table\ncursor.execute('''CREATE TABLE Employees (\n                    ID INTEGER PRIMARY KEY,\n                    Name TEXT,\n                    Department TEXT,\n                    Salary INTEGER)''')\n\n# Create Departments table\ncursor.execute('''CREATE TABLE Departments (\n                    ID INTEGER PRIMARY KEY,\n                    Department_Name TEXT,\n                    Location TEXT)''')\n\n# Insert data into Employees table\nemployees_data = [\n    (1, 'John', 'HR', 50000),\n    (2, 'Alice', 'Engineering', 70000),\n    (3, 'Bob', 'Engineering', 75000),\n    (4, 'Clara', 'HR', 55000),\n    (5, 'David', 'Engineering', 80000)\n]\ncursor.executemany('INSERT INTO Employees VALUES (?, ?, ?, ?)', employees_data)\n\n# Insert data into Departments table\ndepartments_data = [\n    (1, 'HR', 'New York'),\n    (2, 'Engineering', 'San Francisco'),\n    (3, 'Marketing', 'Chicago')\n]\ncursor.executemany('INSERT INTO Departments VALUES (?, ?, ?)', departments_data)\n\nconn.commit()\n\n# Execute the corrected SQL query\ncursor.execute(generated_sql)\n\n# Fetch the result (average salary of employees in departments located in New York)\nresult = cursor.fetchone()\n\n# Print the result\nprint(\"Result:\", result)\n\n# Close the connection\nconn.close()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T08:15:07.450471Z","iopub.execute_input":"2024-11-10T08:15:07.451435Z","iopub.status.idle":"2024-11-10T08:15:35.955878Z","shell.execute_reply.started":"2024-11-10T08:15:07.451361Z","shell.execute_reply":"2024-11-10T08:15:35.954733Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e531a004a5b54393941d24e0a009231e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"805ad5bf9d7b4c72b5c285224d83cf3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27e59c8b625f430888db5c7f4b950166"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17b0893e56fe4ec48d8ffb46829969ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5b583cad7fb4939a931f0d8f86a80f1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Generated SQL Query: SELECT AVG(Employees.Salary) FROM Employees JOIN Departments ON Employees.Department = Departments.Department_Name WHERE Departments.Location = 'New York'\nResult: (52500.0,)\n","output_type":"stream"}],"execution_count":1}]}